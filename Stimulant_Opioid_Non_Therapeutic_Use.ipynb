{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8ec6bb9-5e95-42d4-a663-b21aa468d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#pandas needs 2.0 or higher\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from build_queries import Query\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category = FutureWarning)\n",
    "warnings.filterwarnings('ignore', message = 'This pattern is interpreted as a regular expression,')\n",
    "warnings.filterwarnings('ignore', message = 'pandas only supports SQLAlchemy connectable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75cba1bf-1627-4805-b2af-b5897df363e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "configfile = \"config.txt\"\n",
    "chunksize = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32aa1580-1dfe-47ff-ab53-f724fa7af743",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(configfile)\n",
    "required_headers = [\"INPUT_SETTINGS\", \"SEARCH_SETTINGS\", \"OUTPUT_SETTINGS\"]\n",
    "for req in required_headers:\n",
    "    if req not in config:\n",
    "        raise KeyError(f\"Required configuration option {req} not in config file. See ReadMe for guidance\")\n",
    "\n",
    "input_type = config['INPUT_SETTINGS']['input_type']\n",
    "text_format = config['INPUT_SETTINGS']['text_format']\n",
    "model_indir = config['INPUT_SETTINGS']['model_indir']\n",
    "if model_indir == '' and text_format == 'free text':\n",
    "    raise ValueError(\"Free text is specified but a model path or location is not specified. Free text analysis requires model. See ReadMe for guidance\")\n",
    "cnxn_string = config['INPUT_SETTINGS']['cnxn_string']\n",
    "sql_query = config['INPUT_SETTINGS']['sql_query']\n",
    "infile_path = config['INPUT_SETTINGS']['infile_path']\n",
    "search_terms_path = config['INPUT_SETTINGS']['search_terms_path']\n",
    "\n",
    "col_to_search = config['SEARCH_SETTINGS']['col_to_search']\n",
    "note_type_col = config['SEARCH_SETTINGS']['note_type_col']\n",
    "drugscreen_note_type = config['SEARCH_SETTINGS']['drugscreen_note_type']\n",
    "meds_note_type = config['SEARCH_SETTINGS']['meds_note_type']\n",
    "ehr_diag_titles = config['SEARCH_SETTINGS']['ehr_diag_titles']\n",
    "\n",
    "results_path = config['OUTPUT_SETTINGS']['results_path']\n",
    "cols_to_keep = config['OUTPUT_SETTINGS']['cols_to_keep']\n",
    "group_cols = config['OUTPUT_SETTINGS']['group_cols']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7a02d88-2a9d-43c9-bbe8-41c15078e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All config items read in as strings. Cast anything you want to be something other than string\n",
    "#Input\n",
    "if infile_path == '' and input_type.upper() in ('CSV', 'SAS'):\n",
    "    raise ValueError(\"Input type has been specified as CSV or SAS but no input file path has been specified. See ReadMe\")\n",
    "infile_path = Path(infile_path)\n",
    "if search_terms_path == '':\n",
    "    raise ValueError(\"Search term file must be specified. See ReadMe\")\n",
    "search_terms_path = Path(search_terms_path)\n",
    "\n",
    "#Search Spec\n",
    "if col_to_search == '':\n",
    "    raise ValueError(\"Column to search must be specified\")\n",
    "col_to_search = col_to_search.upper()\n",
    "if text_format == 'FHIR' and (note_type_col == '' or drugscreen_note_type == '' or meds_note_type == '' or ehr_diag_titles == ''):\n",
    "    raise ValueError(\"Text format has been specified as FHIR. Note type column, as well as values in that column indication medications, lab results, \\\n",
    "    and diagnosis-likely values must be specified\")\n",
    "note_type_col = note_type_col.upper()\n",
    "#presumption is that there is one note type each for medications and labs (string), but can be more than one for diagnoses type (list)\n",
    "ehr_diag_titles = [x.strip() for x in ehr_diag_titles.split(',')]\n",
    "\n",
    "#Output Spec\n",
    "if results_path == '':\n",
    "    raise ValueError(\"An output path for results must be specified. See ReadMe\")\n",
    "results_path = Path(results_path)\n",
    "if cols_to_keep == '':\n",
    "    raise ValueError(\"Please specify at least one column to keep in the output, such as a linkage key / unique identifier\")\n",
    "cols_to_keep = [x.strip() for x in cols_to_keep.split(\",\")]\n",
    "\n",
    "if group_cols == '':\n",
    "    group_cols = None\n",
    "else:\n",
    "    group_cols = [x.strip() for x in group_cols.split(\",\")]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88c27a6-1905-4d67-9ea9-65bcdf4bc89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in search terms\n",
    "\n",
    "search_terms = pd.read_excel(Path(search_terms_path), sheet_name = None)\n",
    "stim_term_df = search_terms[\"STIMULANTS\"]     \n",
    "opioid_term_df = search_terms['OPIOIDS']     \n",
    "\n",
    "all_terms_df = pd.concat([stim_term_df, opioid_term_df], sort = False)\n",
    "\n",
    "all_terms_df['Term'] = all_terms_df['Term'].str.strip().str.lower()\n",
    "all_terms_df['Category'] = all_terms_df['Category'].str.strip().str.upper()\n",
    "all_terms_df = all_terms_df[['Term', 'Category']].copy()\n",
    "if all_terms_df['Term'].nunique() != all_terms_df.drop_duplicates().shape[0]:\n",
    "    print('Warning: One or more of the same term appears with more than one category. Only one category will be mapped for each term')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0f32b66-d6c8-45cf-8ce0-4b925d17fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 rows from original dataframe reclassified due to overrides\n"
     ]
    }
   ],
   "source": [
    "#some combinations are so indicative of non-therapeutic use, \n",
    "#you're better off assuming when you see them that that is what they flag\n",
    "\n",
    "overrides_d = {'Term':[], 'Category':[]}\n",
    "\n",
    "for _, row in all_terms_df.iterrows():\n",
    "    k = row.Term\n",
    "    v = row.Category\n",
    "    if v =='RX_OPIOD':\n",
    "        new_v = 'OPIOID_MISUSE_OVERRIDE'\n",
    "    elif v == 'RX_AMPHETAMINE':\n",
    "        new_v = 'STIMULANT_MISUSE_OVERRIDE'\n",
    "    elif v == 'MAT':\n",
    "        new_v = 'OPIOID_MISUSE_OVERRIDE'\n",
    "    elif v == 'UNSPECIFIED_STIMULANT':\n",
    "        new_v = 'STIMULANT_NON_TX_UNSP_OVERRIDE'\n",
    "    elif v == 'UNSPECIFIED_OPIOID':\n",
    "        new_v = 'OPIOID_NON_TX_UNSP_OVERRIDE'\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    abuse_k = f\"{k} abuse\"\n",
    "    dependence_k = f\"{k} dependence\" #no override for MAT for dependence\n",
    "    seeking_k1 = f\"{k}-seeking\"\n",
    "    seeking_k2 = f\"{k} seeking\"\n",
    "    \n",
    "    overrides_d['Term'].append(abuse_k)\n",
    "    overrides_d['Category'].append(new_v)\n",
    "    \n",
    "    overrides_d['Term'].append(seeking_k1)\n",
    "    overrides_d['Category'].append(new_v)\n",
    "    overrides_d['Term'].append(seeking_k2)\n",
    "    overrides_d['Category'].append(new_v)    \n",
    "    if v != 'MAT':\n",
    "        overrides_d['Term'].append(dependence_k)\n",
    "        overrides_d['Category'].append(new_v)\n",
    "\n",
    "overrides = pd.DataFrame(overrides_d)\n",
    "prev_len = len(all_terms_df)\n",
    "all_terms_df = all_terms_df[~all_terms_df.Term.isin(overrides['Term'].values)].copy()\n",
    "override_count = prev_len - len(all_terms_df)\n",
    "print(f\"{override_count} rows from original dataframe reclassified due to overrides\")       \n",
    "all_terms_df = pd.concat([all_terms_df, overrides])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79b03ef2-c73a-411f-90d2-7caa8b3e9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this version of the algorithm, we will not distinguish between different types of prescription stimuulant misuse\n",
    "#That distinction is collapsed here for simplicity further down in the code\n",
    "all_terms_df['Category'] = all_terms_df['Category'].str.replace('RX_AMPHETAMINE', 'RX_STIM')\n",
    "all_terms_df['Category'] = all_terms_df['Category'].str.replace('LISDEXAMFETAMINE', 'RX_STIM') \n",
    "all_terms_df['Category'] = all_terms_df['Category'].str.replace('METHYLPHENIDATE', 'RX_STIM')\n",
    "all_terms_df['Category'] = all_terms_df['Category'].str.replace('RX_COCAINE', 'RX_STIM')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "508833d6-af02-4ec3-8b42-3d301ba9bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully built trie regex query, adding word boundaries, optional s for file:Custom List for 893 items\n"
     ]
    }
   ],
   "source": [
    "#Build the regular expression that will be used to search text. These are the same terms in the master term list\n",
    "\n",
    "query = Query(all_terms_df['Term'].values.tolist(), input_type = \"list\", \n",
    "              query_type = \"boundary with s\")\n",
    "drug_regex = query.build_re()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d85295f-e1c8-4341-bb5a-61bcfaaccb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary that maps the search terms to their categories. Regex was built with automatic optional s for plurals\n",
    "#so add those to the dictionary as well\n",
    "cat_d = {all_terms_df['Term'].values[i].lower().strip() : all_terms_df['Category'].values[i].upper().strip() for i in range(len(all_terms_df))}\n",
    "for term, category in sorted(cat_d.items()):\n",
    "    if not term.endswith('s'):\n",
    "        cat_d[term+\"s\"] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ebdddd0-daa1-4e8f-bfc1-23e128541069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will normalize the text, replacing term for evaluation with DRUGTERM and removing bad characters\n",
    "#that were also removed during model training\n",
    "def normalize_w_drugterm(text, searchterm):\n",
    "    # m = re.search(searchterm, text, flags = re.IGNORECASE)\n",
    "    # if m is None:\n",
    "    #     return(text)\n",
    "    bad_chars = re.compile(r\"Â¶|ï¿½|¶|\\?â€¢|â€¢|Â¶â€¢|ï¿½|Â¶\")\n",
    "    text = re.sub(bad_chars, \" \", text)\n",
    "    text = re.sub(r\" {3,}\", \"  \", text)\n",
    "    match = re.search(searchterm, text, flags = re.IGNORECASE)\n",
    "    # substitution = f\"[drug] {match.group()} [drug]\"\n",
    "    try:\n",
    "        text = re.sub(match.group(), \"DRUGTERM\", text)\n",
    "    except TypeError:\n",
    "        raise Exception(\"Match is: \", match)\n",
    "    except AttributeError:\n",
    "        raise Exception(f\"Searchterm is: {searchterm} and text is {text}\")\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b262065-cf67-483f-9d5b-6e94d8d3db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is written to tokenize in batches\n",
    "\n",
    "def tokenize_and_encode(texts):\n",
    "    max_len = 150\n",
    "    encoded_dict = tokenizer.batch_encode_plus(\n",
    "                        texts,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = max_len,    \n",
    "                        truncation = True, \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   #\n",
    "                        return_tensors = 'pt'    )\n",
    "\n",
    "    return(encoded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc2d782c-0c5a-466f-a211-af64f0352b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the encoded text (input is dictionary)\n",
    "def eval_with_label(encoded_dict):\n",
    "    with torch.no_grad():        \n",
    "        sample_output= model(encoded_dict['input_ids'].to(device), \n",
    "                token_type_ids=None, \n",
    "                attention_mask= encoded_dict['attention_mask'].to(device))\n",
    "        sample_logits = sample_output.logits.detach().cpu().numpy()\n",
    "\n",
    "        \n",
    "    return(sample_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46e84633-0521-45c3-84d6-17956f1119b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function extracts a snippet of text around a drug term. Only this shorter snippets is evaluated \n",
    "#by the model for free text, rather than the whole text:\n",
    "def extract_snippets(text, match_object, window):\n",
    "    snippet_start = max([match_object.start() - window, 0])\n",
    "    snippet_end = min([match_object.end() + window, len(text)])\n",
    "    while snippet_start > 0:\n",
    "        if re.search(r\"[^a-z0-9-]\", text[snippet_start]) is None:\n",
    "            snippet_start -= 1\n",
    "        else:\n",
    "            break\n",
    "    while snippet_end < len(text):\n",
    "        if re.search(r\"[^a-z0-9-]\", text[snippet_end]) is None:\n",
    "            snippet_end += 1\n",
    "        else:\n",
    "            break\n",
    "    snippet = text[snippet_start : snippet_end]    \n",
    "    return((snippet.strip(), match_object.group()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cbddf59-a6a1-467b-bb03-90590f86f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is for FHIR-standard input. Determinations are made at the row level\n",
    "#but each row can have multiple flags\n",
    "def determine_note_flag_FHIR(cats, non_tx):\n",
    "\n",
    "    flags = set()  \n",
    "       \n",
    "    if 'OPIOID_MISUSE_OVERRIDE' in cats:\n",
    "        flags.add('OPIOID_MISUSE_NLP')\n",
    "    if 'STIMULANT_MISUSE_OVERRIDE' in cats:\n",
    "        flags.add('STIM_MISUSE_NLP')\n",
    "    if 'STIMULANT_NON_TX_UNSP_OVERRIDE' in cats:\n",
    "        flags.add('STIM_NON_TX_UNSP_NLP')\n",
    "    if 'OPIOID_NON_TX_UNSP_OVERRIDE' in cats:\n",
    "        flags.add('OPIOID_NON_TX_UNSP_NLP')\n",
    "        \n",
    "        \n",
    "    if 'COCAINE' in cats and non_tx:\n",
    "        flags.add('ILLICIT_COCAINE_NLP')\n",
    "\n",
    "    if 'RX_STIM' in cats: #formerly separate rx_cocaine, rx_amph, methylp, lisdex\n",
    "        if non_tx:\n",
    "            flags.add('STIM_MISUSE_NLP')\n",
    "        else:\n",
    "            flags.add('STIM_TX_NLP')\n",
    "        \n",
    "    if 'METHAMPHETAMINE' in cats:\n",
    "        if non_tx:\n",
    "            flags.add('ILLICIT_METHAMPHETAMINE_NLP')       \n",
    "    \n",
    "    if 'MDMA' in cats:\n",
    "        if non_tx:\n",
    "            flags.add('ILLICIT_MDMA_NLP')\n",
    "        \n",
    "    if 'UNSPECIFIED_STIMULANT' in cats:\n",
    "        if non_tx:\n",
    "            flags.add('STIM_NON_TX_UNSP_NLP')\n",
    "        else:\n",
    "            flags.add('STIM_TX_NLP')\n",
    "        \n",
    "    if 'ILLICIT_OPIOID' in cats:\n",
    "        if non_tx:\n",
    "            flags.add('OPIOID_ILLICIT_NLP')\n",
    "        else:\n",
    "            flags.add('OPIOID_ANY_NLP')            \n",
    "        \n",
    "    if 'RX_OPIOID' in cats:\n",
    "        if non_tx:\n",
    "            flags.add('OPIOID_MISUSE_NLP')\n",
    "        else:\n",
    "            flags.add('OPIOID_ANY_NLP')\n",
    "        \n",
    "    if 'UNSPECIFIED_OPIOID' in cats: \n",
    "        if non_tx:\n",
    "            flags.add('OPIOID_NON_TX_UNSP_NLP')\n",
    "        else:\n",
    "            flags.add('OPIOID_ANY_NLP')   \n",
    "                \n",
    "    if 'FENTANYL' in cats:\n",
    "        if non_tx:\n",
    "            flags.add('OPIOID_ILLICIT_NLP') \n",
    "        else:\n",
    "            flags.add('OPIOID_ANY_NLP')\n",
    "        \n",
    "    if 'MAT' in cats:\n",
    "        if non_tx:\n",
    "            flags.add('OPIOID_MISUSE_NLP')\n",
    "        else:\n",
    "            flags.add('OPIOID_NON_TX_UNSP_NLP')\n",
    "            \n",
    "    return(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3cdc34a-bae5-4a48-b770-d8e3f1e7c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is for free text. Determinations cannot initially be made at the row level but must be made individually\n",
    "#for every mention within the row. Data table is exploded to make each mention its own row. Each row now can only have one flag\n",
    "\n",
    "def determine_note_flag_freetext(cat, non_tx):        \n",
    "        \n",
    "    if cat == 'COCAINE':\n",
    "        if non_tx:\n",
    "            return('ILLICIT_COCAINE_NLP')\n",
    "        else:\n",
    "            return('')\n",
    "\n",
    "    if cat == 'RX_STIM': #formerly separate rx_cocaine, rx_amph, methylp, lisdex\n",
    "        if non_tx:\n",
    "            return('STIM_MISUSE_NLP')\n",
    "        else:\n",
    "            return('STIM_TX_NLP')\n",
    "        \n",
    "    if cat == 'METHAMPHETAMINE':\n",
    "        if non_tx:\n",
    "            return('ILLICIT_METHAMPHETAMINE_NLP')  \n",
    "        else:\n",
    "            return('')\n",
    "    \n",
    "    if cat == 'MDMA':\n",
    "        if non_tx:\n",
    "            return('ILLICIT_MDMA_NLP')\n",
    "        else:\n",
    "            return('')\n",
    "        \n",
    "    if cat == 'UNSPECIFIED_STIMULANT':\n",
    "        if non_tx:\n",
    "            return('STIM_NON_TX_UNSP_NLP')\n",
    "        else:\n",
    "            return('')\n",
    "        \n",
    "    if cat == 'ILLICIT_OPIOID':\n",
    "        if non_tx:\n",
    "            return('OPIOID_ILLICIT_NLP')\n",
    "        else:\n",
    "            return('OPIOID_ANY_NLP')            \n",
    "        \n",
    "    if cat == 'RX_OPIOID':\n",
    "        if non_tx:\n",
    "            return('OPIOID_MISUSE_NLP')\n",
    "        else:\n",
    "            return('OPIOID_ANY_NLP')\n",
    "        \n",
    "    if cat == 'UNSPECIFIED_OPIOID': \n",
    "        if non_tx:\n",
    "            return('OPIOID_NON_TX_UNSP_NLP')\n",
    "        else:\n",
    "            return('OPIOID_ANY_NLP')   \n",
    "                \n",
    "    if cat == 'FENTANYL':\n",
    "        if non_tx:\n",
    "            return('OPIOID_ILLICIT_NLP') \n",
    "        else:\n",
    "            return('OPIOID_ANY_NLP')\n",
    "        \n",
    "    if cat == 'MAT':\n",
    "        if non_tx:\n",
    "            return('OPIOID_MISUSE_NLP')\n",
    "        else:\n",
    "            return('OPIOID_NON_TX_UNSP_NLP')\n",
    "            \n",
    "    #if you get to this point, there is some category that is unaccounted for\n",
    "    if cat is None or cat == '':\n",
    "        return ('')\n",
    "    raise ValueError(f\"This category is unaccounted for in category-to-flag mapping: {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e28aceb9-65eb-4e96-a646-7c42d2a2f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model and tokenizer. Only needed for free text option\n",
    "if text_format == \"free text\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    try: #for local models in a directory\n",
    "        model = BertForSequenceClassification.from_pretrained(str(Path(model_indir)))\n",
    "    except OSError: #for not path-like objects, i.e. call to HuggingFace\n",
    "        model = BertForSequenceClassification.from_pretrained(model_indir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained(str(Path(model_indir)))\n",
    "    except OSError:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_indir)\n",
    "    max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b059f5de-cc6e-437b-aee2-118ab6e6f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output variables for this code defined\n",
    "output_vars = ['STIM_TX_NLP', 'ILLICIT_COCAINE_NLP', 'ILLICIT_METHAMPHETAMINE_NLP',\n",
    "'ILLICIT_MDMA_NLP', 'STIM_MISUSE_NLP', \n",
    "'STIM_NON_TX_UNSP_NLP', 'OPIOID_ANY_NLP', 'OPIOID_ILLICIT_NLP',\n",
    "'OPIOID_MISUSE_NLP', 'OPIOID_NON_TX_UNSP_NLP', 'DRUGSCREEN_NLP']\n",
    "stim_cats = set(['RX_STIM', 'UNSPECIFIED_STIMULANT'])\n",
    "output_vars_df = pd.DataFrame({x: [0] * chunksize for x in output_vars})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a5b800e-a527-48f6-8fd4-dd607b25b8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully built trie regex query, adding word boundaries, optional s for file:Pandas dataframe for 202 items\n"
     ]
    }
   ],
   "source": [
    "#build the regular expressions used to determine drugscreens. They looked different in our FHIR data and free text data\n",
    "#so we built them separately\n",
    "\n",
    "screen_regex_free_text = re.compile(r\"(\\bndet\\b)|(\\^g=)|(urine\\s[neg|pos])|(scr[n\\s]+)|drug screen|tox screen|urine tox|drug abuse screen|drugs of abuse|drugs urin|screen urin|Ur negative|ur positive\", re.I)\n",
    "\n",
    "#screen_regex_ehr = re.compile(r\"drug screen|drugs of abuse|drug abuse screen|drugs urine|drug abuse urine panel|screen urin\", re.I)\n",
    "lab_terms_df = search_terms['LAB_TERMS']\n",
    "lab_terms_df.columns = [x.upper() for x in lab_terms_df.columns]\n",
    "lab_query = Query(lab_terms_df, query_type = 'boundary with s',\n",
    "                  input_type = \"dataframe\")\n",
    "lab_regex = lab_query.build_re()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "526834bc-4db4-4661-9417-aee84455a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1000\n",
    "\n",
    "if input_type.upper() == 'SQL':\n",
    "    if sql_query is None or sql_query == '' or cnxn_string is None or cnxn_string == '':\n",
    "        raise ValueError(\"Input type is SQL but missing either a connection string or a query string\")\n",
    "    cnxn = pyodbc.connect(cnxn_string)\n",
    "    df_iter = pd.read_sql(sql_query, cnxn, chunksize = chunksize)\n",
    "elif input_type.upper() == 'CSV':\n",
    "    if infile_path is None or infile_path == '':\n",
    "        raise ValueError(\"Input type is CSV but missing input file path\")\n",
    "    df_iter = pd.read_csv(Path(infile_path), iterator = True, chunksize = chunksize)\n",
    "elif input_type.upper() == 'SAS':\n",
    "    if infile_path is None or infile_path == '':\n",
    "        raise ValueError(\"Input type is SAS but missing input file path\")\n",
    "    df_iter = pd.read_sas(Path(infile_path), encoding = 'latin-1', iterator = True, chunksize = chunksize)    \n",
    "else:\n",
    "    raise ValueError(\"Input type is not recognized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "183dc694-8605-42b6-b65c-7e640b3df70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_format == \"FHIR\":\n",
    "    fhir_dfs = []\n",
    "    for counter, df in enumerate(df_iter):\n",
    "        print(f\"Processing dataframe {counter}\")\n",
    "        df.columns = df.columns.str.upper()\n",
    "        df = pd.concat([df, output_vars_df.head(len(df))], axis = 1)\n",
    "        if len(df) == 0:    \n",
    "            continue    \n",
    "            \n",
    "        df[col_to_search] = df[col_to_search].fillna('') \n",
    "        df['MATCHES'] = df.apply(lambda row: set([x.lower() for x in re.findall(drug_regex, row[col_to_search])]), axis = 1)\n",
    "        \n",
    "\n",
    "        df['DRUGSCREEN_NLP'] = np.where((df[col_to_search].str.contains(lab_regex, regex = True)) & \\\n",
    "                                            (df[note_type_col] == drugscreen_note_type) , 1, 0) \n",
    "\n",
    "                                                                                \n",
    "        drugscreen_type_df = df.loc[df[note_type_col] == drugscreen_note_type]\n",
    "        #the above is not eligible for further evaluation\n",
    "        df = df.loc[df[note_type_col] != drugscreen_note_type]\n",
    "        \n",
    "        df['CATS'] = df.apply(lambda row: set([cat_d[x.lower()] for x in row['MATCHES']]), axis=1)\n",
    "        df['NON_TX'] = np.where(df[note_type_col].isin(ehr_diag_titles), 1, 0)\n",
    "\n",
    "        #where cats are rx stim cats and note title is medications\n",
    "        df['STIM_TX_NLP'] = df.apply(lambda row: 1 if row[note_type_col] == meds_note_type and \\\n",
    "                                             row['CATS'].intersection(stim_cats) != set() else 0, axis=1)\n",
    "\n",
    "        meds_type_df = df.loc[df[note_type_col] == meds_note_type]\n",
    "        #the above is not eligible for further evaluation\n",
    "        df = df.loc[df[note_type_col] != meds_note_type]\n",
    "                                             \n",
    "        #after determining screenings and tx use, filter out rows that are of no interest \n",
    "        #i.e. rows with no matches\n",
    "        df = df.loc[df['MATCHES'] != set()]\n",
    "        \n",
    "        df['FLAGS'] = df.apply(lambda row: determine_note_flag_FHIR(row['CATS'], row['NON_TX']), axis=1)\n",
    "    \n",
    "        \n",
    "        def update_flags(row):\n",
    "            for flagname in row['FLAGS']:\n",
    "                if flagname not in row.index:\n",
    "                    raise ValueError(f\"Flag {flagname} does not exist in the dataframe\")\n",
    "            row[flagname] = 1\n",
    "            return(row)\n",
    "            \n",
    "        df = df.apply(update_flags, axis = 1)\n",
    "        df = pd.concat([df, drugscreen_type_df, meds_type_df])\n",
    "        df[output_vars] = df[output_vars].fillna(0)\n",
    "        df = df[df[output_vars].max(axis = 1) > 0].copy()\n",
    "        df = df[cols_to_keep + output_vars]\n",
    "        if group_cols is not None and group_cols != '':\n",
    "            df = df.groupby(group_cols, as_index = False).max()\n",
    "        fhir_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daede017-7781-4f6b-820f-7908c4aa48b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing df 0\n"
     ]
    }
   ],
   "source": [
    "if text_format == \"free text\":\n",
    "    free_text_dfs = []\n",
    "    for counter, df in enumerate(df_iter):\n",
    "        #test\n",
    "        print(f\"Processing df {counter}\")\n",
    "        df.columns = df.columns.str.upper()\n",
    "        df[col_to_search] = df[col_to_search].fillna('') \n",
    "        df['MATCHES'] = df.apply(lambda row: set([x.lower() for x in re.findall(drug_regex, row[col_to_search])]), axis = 1)\n",
    "        df = pd.concat([df, output_vars_df.head(len(df))], axis = 1)\n",
    "\n",
    "        #For free text, we don't even consider rows that have no drug terms of interest in them\n",
    "        df = df[df['MATCHES'].apply(lambda x: len(x) > 0)].copy()\n",
    "\n",
    "        \n",
    "        window = 70\n",
    "        #Extract snippet of text around each drug term from our list of drug terms.\n",
    "        #Returns a list of 2-tuples, being the snippet of text and the drugterm in the snippet\n",
    "        df['SNIPPETS'] = df.apply(lambda row: [extract_snippets(row[col_to_search], x, window) for x in \\\n",
    "                                                         re.finditer(drug_regex, row[col_to_search])], axis = 1)\n",
    "        \n",
    "        #Explode list of 2-tuples out so that there is only 1 2-tuple per cell\n",
    "        df = df.explode('SNIPPETS').reset_index(drop=True)\n",
    "\n",
    "        #Normalize text part of 2-tuple, returns (normalized_text, drugterm)\n",
    "        df['NORMALIZED_SNIPPETS'] = df.apply(lambda row: (normalize_w_drugterm(row['SNIPPETS'][0], row['SNIPPETS'][1]),\n",
    "                                                                    row['SNIPPETS'][1]), axis = 1)\n",
    "         #Put normalized text and drug term in diffent columns\n",
    "        df[['NORMALIZED_SNIPPETS', 'MATCH_TERM']] = df['NORMALIZED_SNIPPETS'].apply(lambda x: pd.Series([x[0], x[1]]))\n",
    "\n",
    "        #Drug screens get their own flag, and aren't evaluated further for anything else\n",
    "        df['DRUGSCREEN_NLP'] = np.where(df['NORMALIZED_SNIPPETS'].str.contains(screen_regex_free_text, regex=True), 1, 0)\n",
    "\n",
    "        #Use score questionnaires are pretty standard and don't in and of themselves indicate much. exclude these \n",
    "        df['USE_SCORE'] = np.where(df['NORMALIZED_SNIPPETS'].str.contains(r\"\\bscore\\b\", case=False, regex=True), 1, 0)\n",
    "\n",
    "        #Save these for later\n",
    "        drugscreen_df = df[df['DRUGSCREEN_NLP'] == 1].copy()\n",
    "\n",
    "        #Only rows that are not drugscreens and not use scores will be evaluated\n",
    "        to_eval_df = df[(df['DRUGSCREEN_NLP'] != 1) & (df['USE_SCORE'] != 1)].copy()\n",
    "        \n",
    "        #Get the categories of each match term\n",
    "        to_eval_df['CAT'] = to_eval_df.apply(lambda row: cat_d[row['MATCH_TERM'].lower()] , axis=1)\n",
    "\n",
    "        #Get flags for overrides. These don't need to be evaluated by a model.\n",
    "        to_eval_df['FLAG'] = ''       \n",
    "        to_eval_df['FLAG'] = np.where(to_eval_df['CAT'] == 'OPIOID_MISUSE_OVERRIDE', 'OPIOID_MISUSE_NLP', to_eval_df['FLAG'])\n",
    "        to_eval_df['FLAG'] = np.where(to_eval_df['CAT'] == 'STIMULANT_MISUSE_OVERRIDE', 'STIM_MISUSE_NLP', to_eval_df['FLAG'])\n",
    "        to_eval_df['FLAG'] = np.where(to_eval_df['CAT'] == 'STIMULANT_NON_TX_UNSP_OVERRIDE', 'STIM_NON_TX_UNSP_NLP', to_eval_df['FLAG'])\n",
    "        to_eval_df['FLAG'] = np.where(to_eval_df['CAT'] == 'OPIOID_NON_TX_UNSP_OVERRIDE', 'OPIOID_NON_TX_UNSP_NLP', to_eval_df['FLAG'])\n",
    "\n",
    "        override_df = to_eval_df[to_eval_df['FLAG'] != ''].copy()\n",
    "        to_eval_df = to_eval_df[to_eval_df['FLAG'] == ''].copy()\n",
    "\n",
    "        #Pass to model only those texts the model needs to evaluate for non-therapeutic status\n",
    "        normalized_as_batch = tokenize_and_encode(to_eval_df['NORMALIZED_SNIPPETS'].values.tolist())\n",
    "        classifications = np.argmax(eval_with_label(normalized_as_batch), axis=1)\n",
    "        to_eval_df['NON_TX'] = classifications\n",
    "\n",
    "        #Now that we have a category and a non-therapeutic status, we can determine the flag\n",
    "        to_eval_df['FLAG'] = to_eval_df.apply(lambda row: determine_note_flag_freetext(row['CAT'], row['NON_TX']), axis=1)\n",
    "        #Remove rows with no flags. Everything that remains in FLAG column should equal one of the output variables\n",
    "        to_eval_df = to_eval_df.loc[to_eval_df['FLAG'] != '']\n",
    "\n",
    "        #For each value in FLAG, mark the column with that name as a 1. Add the override_df back in before doing this\n",
    "        to_eval_df = pd.concat([to_eval_df, override_df])\n",
    "        new_rows = []\n",
    "        for _, row in to_eval_df.iterrows():\n",
    "            flag = row['FLAG']\n",
    "            row[flag] = 1\n",
    "            new_row = row.copy()\n",
    "            new_rows.append(new_row)\n",
    "            \n",
    "        to_eval_df = pd.concat(new_rows, axis=1).T\n",
    "\n",
    "        #Now add back the drugscreens, which already have their one flag (DRUGSCREEN_NLP)\n",
    "        df = pd.concat([to_eval_df, drugscreen_df])\n",
    "\n",
    "        #Drop all columns that are not part of our output\n",
    "        df = df[cols_to_keep + output_vars]\n",
    "\n",
    "        #For space, group by here as well. We will have to do this once after searching is done, too.\n",
    "        if group_cols is not None and group_cols != '':\n",
    "            df = df.groupby(group_cols, as_index = False).max()\n",
    "\n",
    "        #Again, for space, let's not keep any rows that don't have any flags\n",
    "        df = df[df[output_vars].max(axis = 1) > 0].copy()\n",
    "        free_text_dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab468b43-7321-403f-903d-e4962844f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_format == \"free text\":\n",
    "    all_dfs = pd.concat(free_text_dfs)\n",
    "else:\n",
    "    all_dfs = pd.concat(fhir_dfs)\n",
    "    \n",
    "if group_cols is not None and group_cols != '':\n",
    "    all_dfs = all_dfs.groupby(group_cols, as_index = False).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac229cb8-25d3-44b3-a7ce-1cf70d5bca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs['STIM_ILLICIT_NLP'] = np.where((all_dfs['ILLICIT_MDMA_NLP'] ==1) | \\\n",
    "        (all_dfs['ILLICIT_COCAINE_NLP'] ==1) | \\\n",
    "        (all_dfs['ILLICIT_METHAMPHETAMINE_NLP']==1), 1, 0)\n",
    "\n",
    "\n",
    "all_dfs['STIM_NON_TX_UNSP_NLP'] = np.where((all_dfs['STIM_MISUSE_NLP'] == 1) | \\\n",
    "        (all_dfs['STIM_ILLICIT_NLP']==1), 0, all_dfs['STIM_NON_TX_UNSP_NLP'])\n",
    "\n",
    "all_dfs['OPIOID_NON_TX_UNSP_NLP'] = np.where((all_dfs['OPIOID_MISUSE_NLP']==1) | \\\n",
    "        (all_dfs['OPIOID_ILLICIT_NLP'] == 1), 0, all_dfs['OPIOID_NON_TX_UNSP_NLP'])\n",
    "\n",
    "all_dfs['STIM_ANY_NON_TX_NLP'] = np.where((all_dfs['STIM_MISUSE_NLP'] == 1) | \\\n",
    "        (all_dfs['STIM_ILLICIT_NLP']==1) | (all_dfs['STIM_NON_TX_UNSP_NLP'] == 1),\n",
    "        1, 0)\n",
    "\n",
    "all_dfs['OPIOID_ANY_NON_TX_NLP'] = np.where((all_dfs['OPIOID_NON_TX_UNSP_NLP'] == 1) | \\\n",
    "        (all_dfs['OPIOID_MISUSE_NLP']==1) | (all_dfs['OPIOID_ILLICIT_NLP'] == 1),\n",
    "        1, 0)\n",
    "\n",
    "all_dfs['STIM_ANY_NLP'] = np.where( (all_dfs['STIM_ANY_NON_TX_NLP'] ==1) | \\\n",
    "        (all_dfs['STIM_TX_NLP']==1), 1, 0)\n",
    "\n",
    "all_dfs['OPIOID_ANY_NLP'] = np.where(all_dfs['OPIOID_ANY_NON_TX_NLP'] ==1, 1, \n",
    "        all_dfs['OPIOID_ANY_NLP'])\n",
    "\n",
    "#We didn't get enough test cases to test the accuracy of some categories. These are dropped here.\n",
    "#The parent columns that they contribute to can remain, however.\n",
    "cols_to_drop = ['STIM_NON_TX_UNSP_NLP', 'STIM_MISUSE_NLP', 'OPIOID_NON_TX_UNSP_NLP', 'OPIOID_MISUSE_NLP']\n",
    "all_dfs.drop(columns = cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d75cfc26-b44c-4c8c-8f18-03cc0f864b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENCOUNTER_ID</th>\n",
       "      <th>EVENT_NUMBER</th>\n",
       "      <th>STIM_TX_NLP</th>\n",
       "      <th>ILLICIT_COCAINE_NLP</th>\n",
       "      <th>ILLICIT_METHAMPHETAMINE_NLP</th>\n",
       "      <th>ILLICIT_MDMA_NLP</th>\n",
       "      <th>OPIOID_ANY_NLP</th>\n",
       "      <th>OPIOID_ILLICIT_NLP</th>\n",
       "      <th>DRUGSCREEN_NLP</th>\n",
       "      <th>STIM_ILLICIT_NLP</th>\n",
       "      <th>STIM_ANY_NON_TX_NLP</th>\n",
       "      <th>OPIOID_ANY_NON_TX_NLP</th>\n",
       "      <th>STIM_ANY_NLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>012Y</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123X</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>456X</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>789Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ENCOUNTER_ID EVENT_NUMBER STIM_TX_NLP ILLICIT_COCAINE_NLP  \\\n",
       "0         012Y            3           1                   0   \n",
       "1         123X            1           0                   0   \n",
       "2         456X            1           0                   0   \n",
       "3         789Z            1           0                   0   \n",
       "\n",
       "  ILLICIT_METHAMPHETAMINE_NLP ILLICIT_MDMA_NLP OPIOID_ANY_NLP  \\\n",
       "0                           0                0              0   \n",
       "1                           0                0              0   \n",
       "2                           0                0              1   \n",
       "3                           1                0              0   \n",
       "\n",
       "  OPIOID_ILLICIT_NLP DRUGSCREEN_NLP  STIM_ILLICIT_NLP  STIM_ANY_NON_TX_NLP  \\\n",
       "0                  0              0                 0                    0   \n",
       "1                  0              1                 0                    0   \n",
       "2                  0              0                 0                    0   \n",
       "3                  0              0                 1                    1   \n",
       "\n",
       "   OPIOID_ANY_NON_TX_NLP  STIM_ANY_NLP  \n",
       "0                      0             1  \n",
       "1                      0             0  \n",
       "2                      1             0  \n",
       "3                      0             1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2519c-f6d9-4a54-a4b7-bdd5bc94000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs.to_csv(Path(results_path), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
